{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the currency\n",
    "#CURRENCY = \"zilliqa\"\n",
    "#CURRENCY_SYMBOL = \"ZIL\"\n",
    "#CURRENCY = \"nexo\"\n",
    "#CURRENCY_SYMBOL = \"NEXO\"\n",
    "CURRENCY = \"bitcoin\"\n",
    "CURRENCY_SYMBOL = \"BTC\"\n",
    "\n",
    "## personal config\n",
    "TWEETS_FOLDER    = f\"data/crypto/{CURRENCY_SYMBOL}\" # Relative path to historical data\n",
    "SEP_CHAR         = '~' # character seperating dates from and to in filename\n",
    "ENVS             = ['CRYPTO', 'LINE_COUNT', 'MOST_RECENT_FILE', 'MOST_RECENT_ID'] # Stored in var.csv\n",
    "MAX_ROW_PER_FILE = 20000 # Each file storing data has a maximum amount of rows\n",
    "\n",
    "tweets_raw_file = f'data/twitter/{CURRENCY_SYMBOL}/{CURRENCY}_tweets_raw.csv'\n",
    "tweets_clean_file = f'data/twitter/{CURRENCY_SYMBOL}/{CURRENCY}_tweets_clean.csv'\n",
    "query = f'#{CURRENCY} OR #{CURRENCY_SYMBOL}' ####TODO PUT BACK  OR {CURRENCY} OR ${CURRENCY} OR ${CURRENCY_SYMBOL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(os.path.dirname(tweets_raw_file)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(tweets_raw_file))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_KEY = '5LCFAyeZfZhfVo1uqWWDEKMdj'\n",
    "APP_SECRET = 'oL6TkV3QHAEiPmMs8PPnNs93g3IFS3pFqS0lKUzU7yeykQZJfT'\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "twitter.get_application_rate_limit_status()['resources']['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_QUERIES = 450\n",
    "data = {\"statuses\": []}\n",
    "next_id = \"998511795781361665\"\n",
    "with open(tweets_raw_file,mode = \"a+\", encoding='utf-8') as f:\n",
    "    if not next_id:\n",
    "        f.write(\"ID,Text,UserName,UserFollowerCount,RetweetCount,Likes,CreatedAt\\n\")\n",
    "    while(True):\n",
    "        twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "        last_size = 0\n",
    "        for i in tqdm(range(NUMBER_OF_QUERIES)):\n",
    "            if not next_id:\n",
    "                data = twitter.search(q=query, lang='en', result_type='recent', count=\"100\") # Use since_id for tweets after id\n",
    "            else:\n",
    "                data[\"statuses\"].extend(twitter.search(q=query, lang='en', result_type='mixed', count=\"100\", max_id=next_id)[\"statuses\"])\n",
    "            if len(data[\"statuses\"]) > 1:\n",
    "                next_id = data[\"statuses\"][len(data[\"statuses\"]) - 1]['id']\n",
    "            if last_size + 1 == len(data[\"statuses\"]):\n",
    "                break\n",
    "            else:\n",
    "                last_size = len(data[\"statuses\"])\n",
    "\n",
    "        print('Retrieved {0}, waiting for 15 minutes until next queries'.format(len(data[\"statuses\"])))\n",
    "        d = pd.DataFrame([[s[\"id\"], s[\"text\"].replace('\\n','').replace('\\r',''), s[\"user\"][\"name\"], s[\"user\"][\"followers_count\"], s[\"retweet_count\"], s[\"favorite_count\"], s[\"created_at\"]] for s in data[\"statuses\"]], columns=('ID', 'Text', 'UserName', \"UserFollowerCount\", 'RetweetCount', 'Likes', \"CreatedAt\"))\n",
    "        d.to_csv(f, mode='a', encoding='utf-8',index=False,header=False)\n",
    "        if last_size + 1 == len(data[\"statuses\"]):\n",
    "            print('No more new tweets, stopping...')\n",
    "            break\n",
    "        data[\"statuses\"] = []\n",
    "        \n",
    "        sleep(910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.dirname(tweets_clean_file)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(tweets_clean_file))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "d = pd.read_csv(tweets_raw_file)\n",
    "for i,s in enumerate(tqdm(d['Text'])):\n",
    "    text = d.loc[i, 'Text']\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('@\\\\w+ *', '', text, flags=re.MULTILINE)\n",
    "    d.loc[i, 'Text'] = text\n",
    "f = open(tweets_clean_file, 'a+', encoding='utf-8')\n",
    "d.to_csv(f, header=True, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# initializing spark session\n",
    "sc = SparkContext(appName=\"PySparkShell\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the schema\n",
    "\n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'ID',          dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'Text',       dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'UserName',       dataType= tp.StringType(),   nullable= True),\n",
    "  tp.StructField(name= 'UserFollowerCount',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'RetweetCount',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'Likes',          dataType= tp.IntegerType(),  nullable= True),\n",
    "  tp.StructField(name= 'CreatedAt',          dataType= tp.StringType(),  nullable= True),\n",
    "])\n",
    "\n",
    "my_data = spark.read.csv(tweets_clean_file,\n",
    "                         schema=my_schema,\n",
    "                         header=True)\n",
    "#view the data\n",
    "my_data.show(5)\n",
    "\n",
    "#print the schema of the file\n",
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1 = RegexTokenizer(inputCol= 'Text' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "# define stage 2: remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "# define stage 3: create a word vector of the size 100\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)\n",
    "# define stage 4: Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "# fit the pipeline model with the training data\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tweet_text):\n",
    "    try:\n",
    "    # filter the tweets whose length is greater than 0\n",
    "        tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
    "    # create a dataframe with column name 'tweet' and each row will contain the tweet\n",
    "        rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
    "    # create a spark dataframe\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "    # transform the data using the pipeline and get the predicted sentiment\n",
    "        pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()\n",
    "    except : \n",
    "        print('No data')\n",
    "    \n",
    "# initialize the streaming context \n",
    "ssc = StreamingContext(sc, batchDuration= 3)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9991\n",
    "lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n",
    "\n",
    "# split the tweet text by a keyword 'TWEET_APP' so that we can identify which set of words is from a single tweet\n",
    "words = lines.flatMap(lambda line : line.split('TWEET_APP'))\n",
    "\n",
    "# get the predicted sentiments for the tweets received\n",
    "words.foreachRDD(get_prediction)\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()             \n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('LSP': conda)",
   "language": "python",
   "name": "python361064bitlspcondad95b3df7e1b14d0b82b89f2d70e6b4d0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
